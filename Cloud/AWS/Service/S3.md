# Amazon S3
* Amazon Simple Storage Service
* Object Storage 서비스
</br>


## S3 개념
* __Bucket__
    * 객체에 대한 컨테이너, S3 라고하면 보통 Bucket을 의미하기도 한다.
* __Object__
    * S3에 저장되는 기본 개체
    * S3에 저장되는 객체는 "<u>__파일 데이터__</u>"와 그 객체를 설명하는 "<u>__메타 데이터__</u>"로 구성
    > 메타데이터 속성을 자유롭게 추가할 수 있다. 그렇기 때문에 빅데이터와 같은 분석 시스템에 잘 어울린다. 하지만 반대로 얘기하면 그만큼 메타 데이터가 차지하는 비율이 크기 때문에 다른 방면에서는 효율적이지 않은 거 같다.
* __Key__
    * 버킷 내 객체의 고유한 식별자. 모든 객체는 하나의 키를 갖는다 (```객체 경로```)
    * S3의 URL, URI, ARN 등은 ```버킷 이름 + 키```로 이루어져 있다.
    > Directory는 이름으로 취급한다. 즉, Console에서 보이는 것은 같은 이름끼리 그룹핑해서 보여주는 것이다!! 그래서 Directory는 Rename이 불가능하다.
* __Region__
    * 버킷을 저장할 지리적 AWS 리전
    * 객체를 다른 리전으로 전송하지 않는 한 해당 리전을 벗어나지 않는다.
* __Consistency Model__
    * 현재 S3도 기본적으로 강력한 일관성(Strong Consistency)을 제공한다. 그렇기 때문에 이제 최신 데이터를 가져오지 않을 수 있는 걱정을 할 필요 없다.
</br>


## S3 기능
* 객체 형태로 Upload 및 Download 기능 제공
* 객체 수에는 제한이 없지만, 1개의 객체는 최대 5TB 까지 제공된다.
* 5GB 이상의 객체에 대해서는 멀티파트 업로드를 이용해야 한다.
* 동일한 객체에 대한 여러 쓰기 요청 시, 마지막 객체를 제외한 모든 객체를 덮어쓴다.
</br>

### S3 장점
* 구조가 단순한 Object 형태로 저장하기 때문에 뛰어난 확장성을 가지고 있다. 기존 파일 시스템은 파일을 검색하려면 계층형 구조를 캐싱해야 되는데, 파일이 많을수록 이를 구현하는데 메모리의 리소스를 많이 소모하기 때문이다. 그렇기 때문에 특히 페타바이트 규모의 환경에서 좋다.
* 읽기 속도가 빠르다.
</br>

### S3 단점
* Object의 메타데이터가 커서, 입출력에서 오버헤드가 발생한다.
* 데이터를 수정하면 Object 자체를 변경해야 하며, 수정이 빈번히 일어나는 경우 비효율적이다.
</br>


## 요청 종류
1. __PUT__
    * 버킷에 Object 추가/변경 요청(Upload). WRITE 권한 필요.
    * HTTP Header를 통해 Parameters 전달
2. __COPY__
    * 다른 버킷으로 복사 요청. 원복 객체에 대한 READ 권한과 대상 버킷에 WRITE 권한이 있어야 한다.
3. __POST__
    * HTML을 이용하여 특정 버킷에 Object 추가/변경 요청. WRITE 권한 필요.
    * multipart/form-data으로 인코딩된 message body를 통해 Parameters 전달
    * 
4. __LIST__
    * 버킷에 있는 Object List 반환 요청
    * 모든 Storage Class에 대한 LIST 요청은 S3 Standard PUT, COPY 및 POST 요청과 같은 요금이 부과된다.
5. __GET__
    * Object 반환 요청(Download). READ 권한 필요.
6. __SELECT__
    * Query 문을 기반으로 S3 객체의 콘텐츠를 필터링하여 일치하는 레코드만 반환
> GET, PUT, LIST가 가장 많이 사용되고, COPY, POST 순으로 사용될거 같다. SELECT는 굳이?
</br>


## S3 종류
1. S3 Standard
    * 기본 S3로 자주 액세스하는 데이터에 사용
        1) Big Data & Analysis Content
        2) Content Distribution
        3) Static Website Hosting
2. S3 Standard - Infrequent Access
    * Live 상태가 오래 되었지만 밀리초 단위 액세스 성능이 요구되지만, 자주 액세스하지 않는 데이터
        1) Backup & Archive
        2) Disaster Recovery
        3) File Sync & Share
        4) Long-retained Data
3. S3 One Zone - Infrequent Access
    * Live 상태가 오래 되었지만 밀리초 단위 액세스 성능이 요구되지만, 자주 액세스하지 않는 데이터
    * 최소 3개 가용 영역에 데이터를 저장하지만 해당 S3는 하나의 가용 영역에 저장하여 S3 Standard - IA보다 20% 저렴하게 사용 가능
4. S3 Glacier Instant Retrieval
    * 분기에 한 번 액세스하는 오래된 아카이브 데이터
5. S3 Glacier Flexible Retrieval
    * 1년에 한 번 액세스하는 오래된 아카이브 데이터
    * 검색 옵션이 1분부터 12시간까지인 장기적인 백업 및 아카이브용
    * 복원하는데 3~5시간이 걸린다.
    * 저장소 생성 및 삭제를 제외하고는 AWS CLI, SDK, API를 사용해야 한다.
        1) Long term achives
        2) Digital preservation
        3) Manetic tape replacement
6. S3 Glacier Deep Archive
    * 일년에 한 번 미만으로 액세스하고 12시간 이내에 복원할 수 있는 장기적인 데이터 아카이빙용
> S3 IA와 One Zone IA의 차이는 가용성의 차이이다. 그렇기 때문에 복구가 가능하거나, 데이터를 다시 생성할 수 있는 경우에는 One Zone IA 사용하고, 가용성이 필요한 경우에는 Standard IA를 사용한다.

> 일반적으로 S3 Standard 30일 -> S3 Standard IA 60dlf -> S3 Glacier 365일 -> 삭제
</br>

### S3 Glacier
* 오랜 기간 저장을 할 것이기 때문에 실제 데이터는 Glacier에 저장하고, 객체 이름 및 기타 메타데이터용으로 8KB의 데이터를 인덱스용으로 S3 Standard에 저장하고 있는다(8KB만큼의 추가 비용이 발생한다!!)
</br>
</br>


---
## 수명주기 정책 (Life Cycle Rule)
* 수명주기가 필요한 경우
    1) Bucket에 주기적으로 Log를 업로드할 경우, 로그는 일정 기간 동안에 필요로 하기 때문에
    2) 특정 기간 동안만 자주 액세스하는 문서. 그 이후에는 실시간으로 액세스가 필요 없지만 특정 기간 동안 보관해야되는 경우
    3) 보관 목적의 데이터. 디지털 미디어, 금융 및 의료 기록, 가공되지 않은 유전체 염기 서열 데이터, 장기 데이터베이스 백업 파일 등
* 작업 종류
    1) 전환 작업 : 스토리지 클래스를 변경한다.
    2) 만료 작업 : 객체의 만료되는 시기를 정한다. 만료된 객체는 자동으로 삭제한다.
* 복원
    * 
</br>

### 수명주기 규칙
* 범위
    1. __모든 객체에 허용__
    2. __하나 이상의 필터를 사용하여 이 규칙의 범위 제한__
* 전환 작업을 위한 보관 최소 기간
    * S3 Standard ---> S3 Standard-IA : 30일 이상
    * S3 Standard ---> S3 One Zone-IA : 30일 이상
    * S3 Standard-IA ---> S3 One Zone-IA : 30일 이상
    * S3 Standard-IA, S3 One Zone-IA ---> S3 Glacier : 30일 이상
    > S3 Glacier 종류가 아닌 경우(S3 Standard, Standard-IA,One Zone-IA), 액세스할 확률이 높기 때문에 최소 30일이 지나야 전환할 수 있다. 변경 전에도 자주 액세스할 확률이 높기 때문에!!
* 제한 사항
    * 128KB보다 작은 객체 - S3 Standard에서 S3 Glacier Flexible Retrieval 또는 S3 Glacier Deep Archive로만 전환할 수 있다. S3 Glacier Instant Retrieval은 128KB 이상 객체를 저장하기 위한 용도로 개발되었다.
    * S3 Glacier 종류가 아닌 경우(S3 Standard, Standard-IA,One Zone-IA), 액세스할 확률이 높기 때문에 최소 30일이 지나야 전환할 수 있다.

</br>


## Version Control
* 버전 관리 기능을 활성화해야 사용할 수 있다.
> 굳이 사용할 필요가 없다고 생각한다. Git을 사용하는 것이 비용적인 측면에서 더 효율적이라고 생각한다.
</br>

## Web hosting
* 정적 웹 호스팅에 사용
</br>


## Security
* 접근제어를 위해, 모든 S3 Public Access 차단 후에 IAM 정책을 통해서 사용. 특정 시나리오에서만 Bucket 정책과 ACL 사용하여 제어한다.
### 1. __S3 액세스 제어__
1) __IAM 정책 (Identity Access Management)__
  * 사용자 기반의 접근제어.
2) __Bucket 정책__
  * 리소스 기반의 접근제어.
  * 사용자 기반의 경우 사용자 별로 전부 설정해야되지만, 리소스 기반의 경우 바로 Public하게 설정할 수 있다.
  * Bucket 정책은 일반적으로 __교차 계정 권한__ 의 경우 버킷 정책을 사용해야 한다. 아래 예제는 해당 Bucket에 대한 권한을 Princiap에 설정되어 있는 계정에 위임한다는 의미이다. 이후부터는 Princiap에 설정되어 있는 계정에서 원하는 리소스 또는 대상에게 권한을 부여하면 된다.
  * Example
    ```
    {
        "Version":"2012-10-17",
        "Statement":[
            {
                "Sid":"AddCannedAcl",
                "Effect":"Allow",
                "Principal": {"AWS": ["arn:aws:iam::111122223333:root","arn:aws:iam::444455556666:root"]},
                "Action":["s3:PutObject","s3:PutObjectAcl"],
                "Resource":"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
                "Condition":{"StringEquals":{"s3:x-amz-acl":["public-read"]}}
            }
        
    }
    ``` 
3) __ACL (Access Control List)__
  * ACL을 통해 액세스를 제어할 수 있다.
  * 객체 ACL: 객체마다 ACL 설정함으로, 해당 객체에 대한 소유권을 제어할 수 있다.
  * 버킷 ACL: 유일하게 권장되는 사용은 Amazon S3 Log Delivery 그룹에 액세스 로그 객체를 버킷에 작성하기 위한 쓰기 권한을 부여하는 것
  > __버킷 소유자가 아닌 다른 접근 가능한 사용자가 객체를 올린 경우, 버킷의 소유자가 해당 객체에 대한 접근이 가능하도록 설정해야 한다!__
4) __CORS (Cross Orgin Resource Sharing)__
  * 한 도메인에서 로드되어 다른 도메인에 있는 리소스와 상호 작용하는 클라이언트 웹 애플리케이션에 대한 방법을 정의
5) __Access Point__
   * 기존에 User가 버킷에 접근할 때 단일 정책(Bucket Policy)으로 수행하던 것을 개별 정책으로 나누어 관리할 수 있게 해주는 서비스이다.
   * 여러 User에 대한 정책을 세우게 되면 Bucket Policy가 너무 커짐으로 분석하기 어렵지만, Access Point를 통해 분리함으로써 관리하기가 편리해진다.
</br>

### 2. __Public Access 차단__
* 기본적으로 새 버킷, 액세스 포인트 및 객체는 Public Access를 허용하지 않는다.
* block_public_acls : ACL에 따른 퍼블릭 액세스 차단 (기존 정책이나 ACL에는 영향 X)
* block_public_policy : 
* ignore_public_acls : true
* restrict_public_buckets : true
</br>

### 3. __Object Lock__
* 고정된 시간 동안 또는 무기한으로 객체의 삭제 또는 덮어쓰기를 방지할 수 있다.
* 해당 옵션은 Versioning Control이 활성화되어 있어야 하며, 버킷 생성시에만 설정할 수 있다.
</br>

### 4. __Encryption__
* KMS를 통해 Key 관리를 하게 된다.
* Server-Side Encrytion 가능
* SSE-S3/SSE-KMS 암호화 설정 가능
    * SSE-S3 : S3 서비스 자체적으로 제공하는 Key (AES-256 사용)
    * SSE-KMS : KMS 서비스를 통해 제공하는 Key (Default: aws/s3 사용)
</br>

### 5. ___Ownership Control__
* 객체 소유권 제어
* 기본적으로 ACL이 비활성화되며, 객체는 해당 계정이 소유한다(권장 사항)
* 필요할시, ACL 활성화 후 변경 가능

> 기본적으로 Public Access는 전부 차단한다.

> S3도 결국 Directory & File 형태이고, Metadata를 통해 File을 Object 형태로 관리하는 서비스

> CORS는 정적 웹 사이트 호스팅으로 사용될 때 사용되기 때문에, 해당 시나리오가 아니면 사용되기 힘들어 보인다.
</br>



## Upload to S3
1. Consol, CLI, SDK, API
    * Console을 이용하여 업로드할 수 있는 파일의 최대 크기는 160GB이고, CLI, SDK, API를 사용하면 5TB 까지 이동시킬 수 있다.
2. AWS Snowball
    * 페타바이트 규모의 데이터 전송
    * 신청하면 Snowball Edge 디바이스가 고객 위치로 배송됨.
3. AWS Snowballmobile
    * 엑사바이트 규모의 데이터 전송
    * 신청하면 세미트레일러 트럭이 온다.
4. Transfer Acceleration
    * CloudFront를 이용하여 전송
    * 전 세계 각지에서 버킷으로 업로드할 수 있다. 
</br>


## Multipart Upload
* 하나의 객체를 여러 파트로 분할하여 일관되게 업로드할 수 있다 (압축 분할을 생각하면 된다)
* __5GB 이상인 경우 사용!!__
* Multipart Uplaod 이점
    1) 개선된 처리량: 파트를 병렬로 업로드하여 처리량이 개선
    2) 네트워크 문제에 대한 빠른 복구: 대용량 객체의 경우 업로드 중간에 네트워크 문제로 인해 끊어지게 되면, 다시 처음부터 전송해야된다.
    3) 객체 업로드 일시 중지 및 재개: 객체 파트를 장시간에 걸처 업로드할 수 있다. 일단 멀티파트 업로드가 시작되면 제한 시간이 없다. 그렇기 때문에 명시적으로 완료되거나 중단해야 된다.
* Multipart Process
    1) 업로드 시작
    2) 파트 업로드 (객체를 파트로 나누어 전송) : 각 파트당 부분 번호를 지정해줘야 한다.
    3) 멀티파트 업로드 완료
    4) 멀티파트 업로드 나열 : 업로드가 완료되면, S3가 개별 파트로부터 전체 객체를 다시 생성한다.

> 대용량 객체를 빠르게 업로드할 때만 사용. 그 외에는 사용하지 않는다. 
</br>


---
## Cost
* https://aws.amazon.com/ko/s3/pricing/
* 사용한 만큼 비용을 지불한다.
* 데이터를 저장하고 관리할 때는 다음과 같은 6가지 비용 요소를 고려해야 한다 (S3 Standard 기준)

1. __스토리지 요금__
    * Object size, 해당 월에 Object를 저장한 기간, Storage Class에 따라 달라진다.
    * $0.025 per GB == 28.84원/GB == 29,532.16원/TB (50TB/Month 기준)
2. __요청 및 데이터 검색 요금__
    * PUT, COPY, POST, LIST, GET, SELECT, 수명 주기 전환 및 데이터 검색 (DELETE 및 CANCEL 요청은 무료)
    * PUT, COPY, POST, LIST, 동일 요금
    * GET, SELECT 및 기타 요청, 동일 요금
    * 수명 주기 전환 요청 요금
    * 데이터 검색 요청 요금
3. __데이터 전송 및 전송 가속화 요금__
    * S3에서 Public 인터넷을 통해 송수신되는 데이터는 요금을 지불한다.
    * 인터넷 -> S3로 데이터 수신의 경우, __무료__ 이다. 전송에 대한 요금만 없을뿐이지 요청 비용이 무료라는 의미.
    * S3 -> 인터넷으로 데이터 송신시 GB당 요금 발생
    * 다음을 제외하고 S3의 모든 송수신 대역폭에 대해 요금을 지불한다.
        * 인터넷에서 전송된 데이터
        * 동일한 AWS 리전의 S3 버킷 간에 전송되는 데이터
        * S3 버킷에서 S3 버킷과 동일한 AWS 리전 내의 AWS 서비스로 전송된 데이터(동일한 AWS 리전의 다른 계정 포함)
        * __Amazon CloudFront로 전송된 데이터__
    *  __월별 500TB를 초과하는 데이터 전송에 대해서는 당사에 문의한다__
4. __데이터 관리 및 분석 요금__
5. __복제에 대한 요금__
6. __S3 객체 Lambda에서 데이터 처리 요금__
</br>

### 절약 방법
* https://aws.amazon.com/ko/premiumsupport/knowledge-center/s3-reduce-costs/
1. 불필요한 멀티파트 업로드를 정리한다.
2. 필요 없는 객체의 이전 버전을 삭제
3. 스토리지 클래스 전환 비용을 검토
4. 데이터 검색 비용을 검토
5. 버킷에 대한 요청을 추적
6. 버킷 크기 변경 사항을 확인
7. 각 버킷의 비용을 검토

> Version Control은 안하는게 좋아보인다. 사용할거면 git으로 버전관리하고 Push하는 방식으로 하는게 좋아 보인다.

> 기본적으로 데이터 검색 비용이 많이들면 EBS같은 것을 사용하는 것이 더 이득으로 보인다.

> S3가 필요한 경우는 Logging용이나 아카이빙같이 수정이 안되는 파일. 그리고 AWS에서 AWS Service to AWS Service로 데이터 전송이 필요한 경우.

> 메타데이터를 커스마이징할 수 있다. 원하는 메타데이터를 정의함으로써 빅데이터와 같은 분석하는 분야에 많이 사용된다고 한다.

> 스토리지 클래스 전환의 경우, 전송 비용은 무료지만 요청은 비용이 부과되기 때문에 요청에 대한 비용 검토가 필요

* SSE-KMS에 저장된 KMS 키로 서버 측 암호화를 사용하여 데이터 보호
